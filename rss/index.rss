<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>Build It Green</title><description>An examination of the LEED certification system.</description><link>http://localhost:2368/</link><generator>Ghost 0.6</generator><lastBuildDate>Fri, 17 Jul 2015 01:43:24 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>APIs and Plotly</title><description>&lt;p&gt;As Mark and I have gotten our workflow down for this project, I have really begun to understand the importance of communication between the back end and front end.  When I’m designing the API endpoints, organization matters, and the closer I can get my endpoints to match the structure&lt;/p&gt;</description><link>http://localhost:2368/apis-and-plotly/</link><guid isPermaLink="false">7016c24e-753f-46cb-a5e3-ebbf7852d71d</guid><dc:creator>josh higgins</dc:creator><pubDate>Fri, 17 Jul 2015 01:43:02 GMT</pubDate><content:encoded>&lt;p&gt;As Mark and I have gotten our workflow down for this project, I have really begun to understand the importance of communication between the back end and front end.  When I’m designing the API endpoints, organization matters, and the closer I can get my endpoints to match the structure mark needs for his visualizations the better. &lt;/p&gt;

&lt;p&gt;Mark and I talked and decided that structuring the API to serve the data better would be faster than him trying to parse a more complicated API.&lt;/p&gt;

&lt;p&gt;To that end, I refactored the API for our breakdown of the credits that LEED projects attain on average.  This is data that has not been presented before, so it’s key that we get it right.  We want to present the data in both yearly trends and show differences between certification levels.  For me that means making some nested dictionaries that can be outputted as JSON for Mark’s D3 visualizations.&lt;/p&gt;

&lt;p&gt;Each data point needs a query similar to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;eqc_dict = {}
for credit in eqc_list:
eqc_dict[credit] = {}
for year in year_list:
    eqc_dict[credit][year] = float("     {0:.2f}".format((ScoreTwoPointTwo.objects
                                        .filter(project__certification_date__gte=datetime(year, 1, 1))
                                        .filter(project__certification_date__lte=datetime(year, 12, 31))
                                        .aggregate(Avg(credit))['{}__avg'.format(credit)]) /
                                   possible_dict[credit]))
for cert in cert_list:
    eqc_dict[credit][cert] = float("{0:.2f}".format((ScoreTwoPointTwo.objects
                                                    .filter(project__certification_level=cert)
                                                    .aggregate(Avg(credit))['{}__avg'.format(credit)]) /
                                                    possible_dict[credit]))
trends_dict["eqc"] = eqc_dict
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This was a pretty complicated structure so I opted to write it out instead of trying to nest dictionary comprehensions.&lt;/p&gt;

&lt;h4&gt;Plotly&lt;/h4&gt;  

&lt;p&gt;In order to get more visualizations on Build It Green, I dug into plot.ly  — an awesome data visualization tool that works really well with Python and pandas.  I really wanted to make a bubble map that displayed LEED projects by city, so I took the morning yesterday and formatted my data to include latitude, longitude, and population for each city, along with a count of projects in each city.  Then I worked in an python notebook and plotly to plot my data.  Check it out:  &lt;a href="https://plot.ly/~jdhiggins/41/leed-projects-1558-681-leed-projects-681-484-leed-projects-505-267-leed-projects/"&gt;https://plot.ly/~jdhiggins/41/leed-projects-1558-681-leed-projects-681-484-leed-projects-505-267-leed-projects/&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title>Weekend Work</title><description>&lt;p&gt;After successfully scraping over 7000 individual scoresheets from the LEED site, we set to work getting our api ready for the display of score information and analysis.&lt;/p&gt;

&lt;p&gt;We believe that we are the first to do a public analysis of individual scoresheets for LEED.  There should be many interesting findings.&lt;/p&gt;</description><link>http://localhost:2368/weekend-work/</link><guid isPermaLink="false">c11a1fe1-2eb3-4623-b56c-5b7c90e55565</guid><dc:creator>josh higgins</dc:creator><pubDate>Mon, 13 Jul 2015 15:09:43 GMT</pubDate><content:encoded>&lt;p&gt;After successfully scraping over 7000 individual scoresheets from the LEED site, we set to work getting our api ready for the display of score information and analysis.&lt;/p&gt;

&lt;p&gt;We believe that we are the first to do a public analysis of individual scoresheets for LEED.  There should be many interesting findings.  We want to start by finding out relatively how difficult it is to achieve each point.  This will be extremely helpful for builders to gauge which points they should be looking to achieve.  We’d also like to highlight the differences in Platinum, Gold, Silver and Certified projects.  What points really make the difference in achieving higher certifications?  Finally, what sort of correlations are there between all the score points.&lt;/p&gt;

&lt;p&gt;This weekend I set up the API to serve up some of these datapoints to our front end to be visualized in D3.  I also ran some initial plot on correlations and averages in ipython notebook with Pandas and seaborn.  Its pretty amazing what you can do with just those tools.  &lt;/p&gt;

&lt;p&gt;Just a sneak peek at what you’ll find when our site goes live:&lt;/p&gt;

&lt;p&gt;Exciting!&lt;/p&gt;

&lt;p&gt;Python tips: &lt;br&gt;
I’ll try to post something here that I thought was fun to learn or know how to do.  I had lots to choose from this weekend, but since I was working in pandas quite a bit, I had a good chance to review and try some new stuff, including reading a postgreSQL table into a dataframe.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import pandas as pd
import psycopg2

conn = psycopg2.connect(database="builditgreen")
import pandas.io.sql as pdsql

scores_2009_df = pdsql.read_frame("SELECT * FROM %s;" % 'api_score2009', conn)
#Note that read_frame is being replaced by read_sql, but either works for now.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pickling &lt;br&gt;
When I get a dataframe imported and in the right shape, I like to pickle it so I can access it later without going through all the steps again.  It can save a lot of time! &lt;br&gt;
First:       &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;average_comparison_2009_small.to_pickle('LEEDv2009byCertificationLevel.pkl')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;df_2009 = pd.read_pickle(‘LEEDv2009byCertificationLevel.pkl’)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To use it again later as df_2009&lt;/p&gt;

&lt;p&gt;Check in again tomorrow, we should have more to talk about as we get into some machine learning and clustering to explore the scoring system.  I’ll also start to look at our analysis of the energy use of LEED projects.&lt;/p&gt;</content:encoded></item><item><title>Checking In</title><description>&lt;p&gt;More APIs…More Data&lt;/p&gt;

&lt;p&gt;We are settled in today and got a little closer to finishing the MVP — a single page website that displays LEED stats by state and trends over time.  Mark has been working on the state map, my side has been to provide him data via our&lt;/p&gt;</description><link>http://localhost:2368/checking-in/</link><guid isPermaLink="false">9ef15942-23fd-42f1-a60b-017be2b5c925</guid><dc:creator>josh higgins</dc:creator><pubDate>Mon, 13 Jul 2015 01:13:00 GMT</pubDate><content:encoded>&lt;p&gt;More APIs…More Data&lt;/p&gt;

&lt;p&gt;We are settled in today and got a little closer to finishing the MVP — a single page website that displays LEED stats by state and trends over time.  Mark has been working on the state map, my side has been to provide him data via our Django REST framework API.  Since states are a model in our database, I used a generic ListAPIView for GET requests about the states.  I wrote a serializer that would display info for each state.&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; class StateMapSerializer(serializers.ModelSerializer):
     number_of_projects =        serializers.SerializerMethodField()
 class Meta:
     model = State

 fields = ('abbreviation', 'name', 'number_of_projects',

 def get_number_of_projects(self, obj):
     return obj.project_set.all().count()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For each state, we’ll have a bunch of queries that are serialized that Mark can display on a US map.&lt;/p&gt;

&lt;p&gt;For now, he can access all the state data through one api URL.&lt;/p&gt;

&lt;p&gt;API Views &lt;br&gt;
Generic views and viewsets can be great, but if you need to get some basic queries to the API, using a regular APIView does the trick.&lt;/p&gt;

&lt;p&gt;It was good review to some of these for the trend data by year for LEED projects.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class AllTrends(APIView):

def get(self, request, format=None):
    """
    Return trends by year.
    """
    trends_dict = {}
    year_list = [2009, 2010, 2011, 2012, 2013, 2014]
    projects_certified_dict = {i: Project.objects
       .filter(certification_date__gte=datetime(i, 1,\                                 
       1)).filter(certification_date__lte=datetime(i,\       
       12,31)).count() for i in year_list}
    trends_dict["total_certifications"] =   
       projects_certified_dict

 return Response(trends_dict)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then link this up to a URL and it is ready for the front end.&lt;/p&gt;

&lt;p&gt;We have a few more queries to write for trend display— then our back end will be ready for our MVP of state and yearly trends.  That means it is time for...&lt;/p&gt;

&lt;p&gt;More Data &lt;br&gt;
We got our scraper to work on the LEED site!  We’ve got detailed score data for about 7000 projects to do some in depth analysis on how builders are using the system. &lt;/p&gt;

&lt;p&gt;Which points are easiest to attain?  Which are hardest?  Do project total scores cluster around the cutoffs for each certification level?  We’ll spend the next couple days doing analysis as we finish up the MVP.  Then we’ll have our second phase of visualizations to display.&lt;/p&gt;

&lt;p&gt;Check in with us again and see how we’re doing!&lt;/p&gt;</content:encoded></item><item><title>A day of REST</title><description>&lt;p&gt;Today we started to get rolling on our MVP - a data visualization of LEED projects in the United States.  On the front end we plan to have an interactive map visualizing the LEED project totals for each state with more granular data accessible by hovering.&lt;/p&gt;

&lt;p&gt;On the back end&lt;/p&gt;</description><link>http://localhost:2368/untita-day-of-restled/</link><guid isPermaLink="false">550ae3ab-e15d-4e1c-9d17-1b28daff4cb7</guid><dc:creator>josh higgins</dc:creator><pubDate>Thu, 09 Jul 2015 02:44:44 GMT</pubDate><content:encoded>&lt;p&gt;Today we started to get rolling on our MVP - a data visualization of LEED projects in the United States.  On the front end we plan to have an interactive map visualizing the LEED project totals for each state with more granular data accessible by hovering.&lt;/p&gt;

&lt;p&gt;On the back end that means we need to have an API that can communicate with our node front end.  Throughout class we have worked with the Django REST framework.  Today I worked on implementing the first views, serializers and urls for our site.  The basics of REST as we have implemented at the Iron Yard are that URLs are nouns, HTTP actions are verbs (GET, POST, PATCH, PUT, DELETE).  Each query is self-contained and stateless.&lt;/p&gt;

&lt;p&gt;Our MVP only needs PUT functionality, so I chose an generic ListAPIView for a list of states at the URL api/states/.  The serializer defined a number of SerializerMethodFields to render our data to JSON. For example, I want an average of LEED scores for the latest version of LEED.  First, I made a serializer:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;average_of_leed_nc_v2009 = serializers.SerializerMethodField()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, define the output for the serializer:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_average_of_leed_nc_v2009(self, obj):
if not obj.project_set.filter(leed_version="LEED-NC v2009")\
        .aggregate(Avg('points_achieved'))['points_achieved__avg']:
    return 0
else:
    return int(round(obj.project_set.filter(leed_version="LEED-NC v2009")\
        .aggregate(Avg('points_achieved'))['points_achieved__avg']))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We had some null values that are converted to 0 with the if statement.  Otherwise, we’ll take the rounded average for display on the site.&lt;/p&gt;

&lt;p&gt;We did something similar for most of our state data and we are ready to visualize.&lt;/p&gt;

&lt;p&gt;Python Tips &lt;br&gt;
We’re also still cleaning some data.  I had a particularly tricky small csv containing building permit stats for each state over the last 15 years.  Each column alternated between two different data, so I had to put data from the odd columns into one model and even columns into another.  I was pretty happy with my solution:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def make_state_building_permits():

with open("states_building_permits.csv") as infile:
    reader = csv.reader(infile)
    for row in reader:

        for i in range(len(row)):

            if i == 0:
                print(row[0])

            elif i % 2 != 0:
                building_permit = BuildingPermit()
                building_permit.state_id = State.objects.get(abbreviation = col_headers[i])
                building_permit.year = row[0]
                building_permit.total = row[i]
                building_permit.save()

            else:
                housing_permit = HousingPermit()
                housing_permit.state_id = State.objects.get(abbreviation = col_headers[i])
                housing_permit.year = row[0]
                housing_permit.total = row[i]
                housing_permit.save()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(note: col_headers was a list of state abbreviations that corresponded to the csv headers)&lt;/p&gt;

&lt;p&gt;Check in tomorrow for some more progress.  We are hoping to start Phase 2 of our data collection — digging into some individual scores for specific buildings.&lt;/p&gt;</content:encoded></item><item><title>Pivot Day</title><description>&lt;p&gt;Today our team got together and decided to pivot our project goals.  With a two week deadline, we needed to make sure we could deliver a compelling, viable product on demo day.&lt;/p&gt;

&lt;p&gt;After a quick morning meeting, we decided to move on to a project with huge data science and&lt;/p&gt;</description><link>http://localhost:2368/pivot-day/</link><guid isPermaLink="false">69a6b6f5-6955-45b5-b075-a7169e8fd218</guid><dc:creator>josh higgins</dc:creator><pubDate>Wed, 08 Jul 2015 02:47:00 GMT</pubDate><content:encoded>&lt;p&gt;Today our team got together and decided to pivot our project goals.  With a two week deadline, we needed to make sure we could deliver a compelling, viable product on demo day.&lt;/p&gt;

&lt;p&gt;After a quick morning meeting, we decided to move on to a project with huge data science and visualization potential:  the LEED Certification process.  &lt;/p&gt;

&lt;p&gt;LEED &lt;br&gt;
LEED is a is a set of rating systems for the design, construction, operation, and maintenance of green buildings, homes, and neighborhoods.   You can read more at:  &lt;a href="http://www.usgbc.org/certification"&gt;http://www.usgbc.org/certification&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We plan to make data on all existing LEED projects consumable and interesting for anyone interested in the project.  Our goals are to visualize the distribution of LEED projects in the US, analyze the scoring system and how builders choose which points to attain, and do some case analysis of individual buildings and their performance.  These smaller case studies are a stretch goal.&lt;/p&gt;

&lt;p&gt;Daily Progress &lt;br&gt;
We formulated our project and set up our Trello board.  This project will be perfect for Agile project management, including daily standups. After setting up our Django project, deploying to Heroku, and setting up our models and our PostgreSQL database, I worked on loading the initial LEED data to the database.  The overview data for LEED projects is available in a csv.  For individual projects, we’ll need to scrape the LEED site, but to do our initial analysis the overview data is fine.&lt;/p&gt;

&lt;p&gt;The bulk of the afternoon and evening was spent wrangling the csv data into the database.  Even when a csv looks pretty clean, there are always problems with formatting.  That’s part of the job, and its fun to tackle each problem one by one until you have a clean set.&lt;/p&gt;

&lt;p&gt;Wrangling the data also gives a good review of python and how it can handle different problems.  I’ll try to show a few code snippets each day that I find interesting.&lt;/p&gt;

&lt;p&gt;Python Tips &lt;br&gt;
The date records for certifications are modeled as django DateField, but our csv came in the format ’ 10/25/00’, and sometimes the field was empty.  To load this correctly I wrote a function using strptime, which parses a string representing a time according to a format.   In our example, we needed to use:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%y - year without a century (range 00 to 99)
%m - month (01 to 12)
%d - day of the month (01 to 31)

import time
from datetime import date

def fix_time(time_string):
    if time_string == "":
        return None
else:
    strp_time = time.strptime(time_string, '%m/%d/%y')
    return date.fromtimestamp(time.mktime(strp_time))
&lt;/code&gt;&lt;/pre&gt;</content:encoded></item><item><title>Data Visualization - Day One</title><description>&lt;p&gt;Today, I focused primarily on preparing the front end structure of this project. &lt;/p&gt;

&lt;p&gt;To start, I setup this blog to track our daily progress. It is built on the node-based blogging platform &lt;a href="http://localhost:2368/data-visualization-day-one/ghost.org"&gt;Ghost.io&lt;/a&gt; and hosted on Github via a static page generator, &lt;a href="https://github.com/axitkhurana/buster"&gt;Buster&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The next step was to explore&lt;/p&gt;</description><link>http://localhost:2368/data-visualization-day-one/</link><guid isPermaLink="false">db7feddd-51ae-48da-9857-d8fd6db5e553</guid><dc:creator>Mark Harper</dc:creator><pubDate>Tue, 07 Jul 2015 05:16:05 GMT</pubDate><content:encoded>&lt;p&gt;Today, I focused primarily on preparing the front end structure of this project. &lt;/p&gt;

&lt;p&gt;To start, I setup this blog to track our daily progress. It is built on the node-based blogging platform &lt;a href="http://localhost:2368/data-visualization-day-one/ghost.org"&gt;Ghost.io&lt;/a&gt; and hosted on Github via a static page generator, &lt;a href="https://github.com/axitkhurana/buster"&gt;Buster&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The next step was to explore the javascript data visualization library &lt;a href="http://localhost:2368/data-visualization-day-one/d3js.org"&gt;D3 JS&lt;/a&gt;. I started by importing a dataset that Josh provided. This dataset shows the frequency of operating systems used in the user population from the &lt;a href="http://localhost:2368/data-visualization-day-one/kaggle.com/c/icdm-2015-drawbridge-cross-device-connections"&gt;Kaggle Competition&lt;/a&gt;. The biggest challenge that I encountered in visualizing the data was choosing how to best reorganize the dataset to easily build a bar chart in D3. I found that building a bar chart was easiest when each entry was formatted as a JSON object with an id and frequency. &lt;/p&gt;

&lt;p&gt;There are a few remaining problems with the bar chart as it stands now. It is not responsive, it cuts off entries beyond 56, it lacks a color gradient, and it is not interactive. The above features are all things that I am going to implement prior to demo day. So much left to do in the days to come. Check out the bar chart below!&lt;/p&gt;

&lt;div class="os-chart"&gt;&lt;/div&gt;</content:encoded></item><item><title>Data Analysis - Day One</title><description>&lt;p&gt;Today was a day to further explore the data and test out some initial ideas.&lt;/p&gt;

&lt;p&gt;We encountered a few problems that will help us define the next steps for analysis.&lt;/p&gt;

&lt;p&gt;Our first task, as always, is to define our problem.  We can view this as a similarity matching problem.  In&lt;/p&gt;</description><link>http://localhost:2368/data-analysis-day-one/</link><guid isPermaLink="false">7c22130d-5b12-4b5c-87ca-6017f9253295</guid><dc:creator>josh higgins</dc:creator><pubDate>Tue, 07 Jul 2015 03:15:05 GMT</pubDate><content:encoded>&lt;p&gt;Today was a day to further explore the data and test out some initial ideas.&lt;/p&gt;

&lt;p&gt;We encountered a few problems that will help us define the next steps for analysis.&lt;/p&gt;

&lt;p&gt;Our first task, as always, is to define our problem.  We can view this as a similarity matching problem.  In many ways it is like our movie review recommendation project.  &lt;a href="https://github.com/tiyd-python-2015-05/movie-recommendations"&gt;https://github.com/tiyd-python-2015-05/movie-recommendations&lt;/a&gt;  We can vectorize the shared characteristics of the devices and the cookies and use cosine similarity to score their similarity.  Hopefully the similarity will give us decent predictions.&lt;/p&gt;

&lt;p&gt;One roadblock is categorical data.  To properly vectorize this type of data we have to make a dummy column for each possible categorical value.  In the case of IP address and some of the anonymous categorical data where there can be thousands of different matches, this has hindered our progress.&lt;/p&gt;

&lt;p&gt;Headway was made when doing similarity scores based on country code and numerical data.  Our first attempts can be seen in this IPython Notebook: &lt;a href="https://github.com/Data-Science-TIY/data-science/blob/master/Make%20Cookie%20Dummies.ipynb"&gt;https://github.com/Data-Science-TIY/data-science/blob/master/Make%20Cookie%20Dummies.ipynb&lt;/a&gt;.  Our first test prediction matched a phone and device that had matching scores for Anonymous 5, 6, and 7 values.  They did not have matching country ids, so the prediction was wrong.  Perhaps filtering the list for country id before attempting a match would be a good place to start tomorrow.  &lt;/p&gt;

&lt;p&gt;Another idea is to filter by shared IP address.  With a dataframe of over 30M instances, using dummy columns to do a cosine similarity will be difficult.  We might first try to filter by shared IP so that there are fewer choices for each match of device to cookie.&lt;/p&gt;

&lt;p&gt;Lots to think about, and lots to do!&lt;/p&gt;</content:encoded></item></channel></rss>