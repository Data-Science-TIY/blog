<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>Build It Green</title><description>An examination of the LEED certification system.</description><link>http://localhost:2368/</link><generator>Ghost 0.6</generator><lastBuildDate>Thu, 09 Jul 2015 13:04:35 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>A day of REST</title><description>&lt;p&gt;Today we started to get rolling on our MVP - a data visualization of LEED projects in the United States.  On the front end we plan to have an interactive map visualizing the LEED project totals for each state with more granular data accessible by hovering.&lt;/p&gt;

&lt;p&gt;On the back end&lt;/p&gt;</description><link>http://localhost:2368/untita-day-of-restled/</link><guid isPermaLink="false">550ae3ab-e15d-4e1c-9d17-1b28daff4cb7</guid><dc:creator>Mark Harper</dc:creator><pubDate>Thu, 09 Jul 2015 02:44:44 GMT</pubDate><content:encoded>&lt;p&gt;Today we started to get rolling on our MVP - a data visualization of LEED projects in the United States.  On the front end we plan to have an interactive map visualizing the LEED project totals for each state with more granular data accessible by hovering.&lt;/p&gt;

&lt;p&gt;On the back end that means we need to have an API that can communicate with our node front end.  Throughout class we have worked with the Django REST framework.  Today I worked on implementing the first views, serializers and urls for our site.  The basics of REST as we have implemented at the Iron Yard are that URLs are nouns, HTTP actions are verbs (GET, POST, PATCH, PUT, DELETE).  Each query is self-contained and stateless.&lt;/p&gt;

&lt;p&gt;Our MVP only needs PUT functionality, so I chose an generic ListAPIView for a list of states at the URL api/states/.  The serializer defined a number of SerializerMethodFields to render our data to JSON. For example, I want an average of LEED scores for the latest version of LEED.  First, I made a serializer:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;average_of_leed_nc_v2009 = serializers.SerializerMethodField()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, define the output for the serializer:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def get_average_of_leed_nc_v2009(self, obj):
if not obj.project_set.filter(leed_version="LEED-NC v2009")\
        .aggregate(Avg('points_achieved'))['points_achieved__avg']:
    return 0
else:
    return int(round(obj.project_set.filter(leed_version="LEED-NC v2009")\
        .aggregate(Avg('points_achieved'))['points_achieved__avg']))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We had some null values that are converted to 0 with the if statement.  Otherwise, we’ll take the rounded average for display on the site.&lt;/p&gt;

&lt;p&gt;We did something similar for most of our state data and we are ready to visualize.&lt;/p&gt;

&lt;p&gt;Python Tips &lt;br&gt;
We’re also still cleaning some data.  I had a particularly tricky small csv containing building permit stats for each state over the last 15 years.  Each column alternated between two different data, so I had to put data from the odd columns into one model and even columns into another.  I was pretty happy with my solution:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def make_state_building_permits():

with open("states_building_permits.csv") as infile:
    reader = csv.reader(infile)
    for row in reader:

        for i in range(len(row)):

            if i == 0:
                print(row[0])

            elif i % 2 != 0:
                building_permit = BuildingPermit()
                building_permit.state_id = State.objects.get(abbreviation = col_headers[i])
                building_permit.year = row[0]
                building_permit.total = row[i]
                building_permit.save()

            else:
                housing_permit = HousingPermit()
                housing_permit.state_id = State.objects.get(abbreviation = col_headers[i])
                housing_permit.year = row[0]
                housing_permit.total = row[i]
                housing_permit.save()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(note: col_headers was a list of state abbreviations that corresponded to the csv headers)&lt;/p&gt;

&lt;p&gt;Check in tomorrow for some more progress.  We are hoping to start Phase 2 of our data collection — digging into some individual scores for specific buildings.&lt;/p&gt;</content:encoded></item><item><title>Pivot Day</title><description>&lt;p&gt;Today our team got together and decided to pivot our project goals.  With a two week deadline, we needed to make sure we could deliver a compelling, viable product on demo day.&lt;/p&gt;

&lt;p&gt;After a quick morning meeting, we decided to move on to a project with huge data science and&lt;/p&gt;</description><link>http://localhost:2368/pivot-day/</link><guid isPermaLink="false">69a6b6f5-6955-45b5-b075-a7169e8fd218</guid><dc:creator>Mark Harper</dc:creator><pubDate>Wed, 08 Jul 2015 02:47:00 GMT</pubDate><content:encoded>&lt;p&gt;Today our team got together and decided to pivot our project goals.  With a two week deadline, we needed to make sure we could deliver a compelling, viable product on demo day.&lt;/p&gt;

&lt;p&gt;After a quick morning meeting, we decided to move on to a project with huge data science and visualization potential:  the LEED Certification process.  &lt;/p&gt;

&lt;p&gt;LEED &lt;br&gt;
LEED is a is a set of rating systems for the design, construction, operation, and maintenance of green buildings, homes, and neighborhoods.   You can read more at:  &lt;a href="http://www.usgbc.org/certification"&gt;http://www.usgbc.org/certification&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We plan to make data on all existing LEED projects consumable and interesting for anyone interested in the project.  Our goals are to visualize the distribution of LEED projects in the US, analyze the scoring system and how builders choose which points to attain, and do some case analysis of individual buildings and their performance.  These smaller case studies are a stretch goal.&lt;/p&gt;

&lt;p&gt;Daily Progress &lt;br&gt;
We formulated our project and set up our Trello board.  This project will be perfect for Agile project management, including daily standups. After setting up our Django project, deploying to Heroku, and setting up our models and our PostgreSQL database, I worked on loading the initial LEED data to the database.  The overview data for LEED projects is available in a csv.  For individual projects, we’ll need to scrape the LEED site, but to do our initial analysis the overview data is fine.&lt;/p&gt;

&lt;p&gt;The bulk of the afternoon and evening was spent wrangling the csv data into the database.  Even when a csv looks pretty clean, there are always problems with formatting.  That’s part of the job, and its fun to tackle each problem one by one until you have a clean set.&lt;/p&gt;

&lt;p&gt;Wrangling the data also gives a good review of python and how it can handle different problems.  I’ll try to show a few code snippets each day that I find interesting.&lt;/p&gt;

&lt;p&gt;Python Tips &lt;br&gt;
The date records for certifications are modeled as django DateField, but our csv came in the format ’ 10/25/00’, and sometimes the field was empty.  To load this correctly I wrote a function using strptime, which parses a string representing a time according to a format.   In our example, we needed to use:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%y - year without a century (range 00 to 99)
%m - month (01 to 12)
%d - day of the month (01 to 31)

import time
from datetime import date

def fix_time(time_string):
    if time_string == "":
        return None
else:
    strp_time = time.strptime(time_string, '%m/%d/%y')
    return date.fromtimestamp(time.mktime(strp_time))
&lt;/code&gt;&lt;/pre&gt;</content:encoded></item><item><title>Data Visualization - Day One</title><description>&lt;p&gt;Today, I focused primarily on preparing the front end structure of this project. &lt;/p&gt;

&lt;p&gt;To start, I setup this blog to track our daily progress. It is built on the node-based blogging platform &lt;a href="http://localhost:2368/data-visualization-day-one/ghost.org"&gt;Ghost.io&lt;/a&gt; and hosted on Github via a static page generator, &lt;a href="https://github.com/axitkhurana/buster"&gt;Buster&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The next step was to explore&lt;/p&gt;</description><link>http://localhost:2368/data-visualization-day-one/</link><guid isPermaLink="false">db7feddd-51ae-48da-9857-d8fd6db5e553</guid><dc:creator>Mark Harper</dc:creator><pubDate>Tue, 07 Jul 2015 05:16:05 GMT</pubDate><content:encoded>&lt;p&gt;Today, I focused primarily on preparing the front end structure of this project. &lt;/p&gt;

&lt;p&gt;To start, I setup this blog to track our daily progress. It is built on the node-based blogging platform &lt;a href="http://localhost:2368/data-visualization-day-one/ghost.org"&gt;Ghost.io&lt;/a&gt; and hosted on Github via a static page generator, &lt;a href="https://github.com/axitkhurana/buster"&gt;Buster&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The next step was to explore the javascript data visualization library &lt;a href="http://localhost:2368/data-visualization-day-one/d3js.org"&gt;D3 JS&lt;/a&gt;. I started by importing a dataset that Josh provided. This dataset shows the frequency of operating systems used in the user population from the &lt;a href="http://localhost:2368/data-visualization-day-one/kaggle.com/c/icdm-2015-drawbridge-cross-device-connections"&gt;Kaggle Competition&lt;/a&gt;. The biggest challenge that I encountered in visualizing the data was choosing how to best reorganize the dataset to easily build a bar chart in D3. I found that building a bar chart was easiest when each entry was formatted as a JSON object with an id and frequency. &lt;/p&gt;

&lt;p&gt;There are a few remaining problems with the bar chart as it stands now. It is not responsive, it cuts off entries beyond 56, it lacks a color gradient, and it is not interactive. The above features are all things that I am going to implement prior to demo day. So much left to do in the days to come. Check out the bar chart below!&lt;/p&gt;

&lt;div class="os-chart"&gt;&lt;/div&gt;</content:encoded></item><item><title>Data Analysis - Day One</title><description>&lt;p&gt;Today was a day to further explore the data and test out some initial ideas.&lt;/p&gt;

&lt;p&gt;We encountered a few problems that will help us define the next steps for analysis.&lt;/p&gt;

&lt;p&gt;Our first task, as always, is to define our problem.  We can view this as a similarity matching problem.  In&lt;/p&gt;</description><link>http://localhost:2368/data-analysis-day-one/</link><guid isPermaLink="false">7c22130d-5b12-4b5c-87ca-6017f9253295</guid><dc:creator>Mark Harper</dc:creator><pubDate>Tue, 07 Jul 2015 03:15:05 GMT</pubDate><content:encoded>&lt;p&gt;Today was a day to further explore the data and test out some initial ideas.&lt;/p&gt;

&lt;p&gt;We encountered a few problems that will help us define the next steps for analysis.&lt;/p&gt;

&lt;p&gt;Our first task, as always, is to define our problem.  We can view this as a similarity matching problem.  In many ways it is like our movie review recommendation project.  &lt;a href="https://github.com/tiyd-python-2015-05/movie-recommendations"&gt;https://github.com/tiyd-python-2015-05/movie-recommendations&lt;/a&gt;  We can vectorize the shared characteristics of the devices and the cookies and use cosine similarity to score their similarity.  Hopefully the similarity will give us decent predictions.&lt;/p&gt;

&lt;p&gt;One roadblock is categorical data.  To properly vectorize this type of data we have to make a dummy column for each possible categorical value.  In the case of IP address and some of the anonymous categorical data where there can be thousands of different matches, this has hindered our progress.&lt;/p&gt;

&lt;p&gt;Headway was made when doing similarity scores based on country code and numerical data.  Our first attempts can be seen in this IPython Notebook: &lt;a href="https://github.com/Data-Science-TIY/data-science/blob/master/Make%20Cookie%20Dummies.ipynb"&gt;https://github.com/Data-Science-TIY/data-science/blob/master/Make%20Cookie%20Dummies.ipynb&lt;/a&gt;.  Our first test prediction matched a phone and device that had matching scores for Anonymous 5, 6, and 7 values.  They did not have matching country ids, so the prediction was wrong.  Perhaps filtering the list for country id before attempting a match would be a good place to start tomorrow.  &lt;/p&gt;

&lt;p&gt;Another idea is to filter by shared IP address.  With a dataframe of over 30M instances, using dummy columns to do a cosine similarity will be difficult.  We might first try to filter by shared IP so that there are fewer choices for each match of device to cookie.&lt;/p&gt;

&lt;p&gt;Lots to think about, and lots to do!&lt;/p&gt;</content:encoded></item></channel></rss>